import os
import json
import asyncio
import logging
import shutil
import aiofiles

from pathlib import Path
from typing import List, Tuple
from datetime import datetime
from functools import lru_cache

import streamlit as st
import sqlite3
from dotenv import load_dotenv
from openai import OpenAI, APIError
from docx import Document
from PyPDF2 import PdfReader
from duckduckgo_search import DDGS
import easyocr
from pdf2image import convert_from_path
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import warnings

warnings.filterwarnings("ignore")
load_dotenv()

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)

CONFIG = {
    "app_name": "AI Document Assistant Pro",
    "max_context_tokens": 4000,  # –ª–∏–º–∏—Ç –ø–æ —Å–∏–º–≤–æ–ª–∞–º
    "chunk_size": 500,
    "top_k": 3,
    "embedding_model": "paraphrase-multilingual-MiniLM-L12-v2",
    "openrouter_model": "meta-llama/llama-3.2-3b-instruct:free",
    "documents_dir": "documents",
    "answers_dir": "–û—Ç–≤–µ—Ç—ã –ò–ò-–∞–≥–µ–Ω—Ç–∞",
    "db_path": "knowledge_base_pro.db",
    "poppler_path": shutil.which("pdftoppm"),  # –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ poppler
    "retry_attempts": 3,
    "ocr_cache": True,

    # –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–µ–±-–ø–æ–∏—Å–∫–∞
    "use_web_search_default": True,
    "min_doc_context_chars": 500,   # –ø–æ—Ä–æ–≥ –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (—Å–∏–º–≤–æ–ª—ã)
    "min_doc_chunks": 1,            # –ø–æ—Ä–æ–≥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —á–∞–Ω–∫–æ–≤ –≤ –ë–ó
    "web_max_results": 3,           # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ DuckDuckGo
}


def load_config() -> dict:
    config_path = Path("config.json")
    if config_path.exists():
        with open(config_path, "r", encoding="utf-8") as f:
            user_config = json.load(f)
        CONFIG.update(user_config)

    os.makedirs(CONFIG["documents_dir"], exist_ok=True)
    os.makedirs(CONFIG["answers_dir"], exist_ok=True)
    return CONFIG


CONFIG = load_config()


class OpenRouterClient:
    def __init__(self) -> None:
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.model_id = os.getenv("ID_MODEL", CONFIG["openrouter_model"])
        if not self.api_key:
            logging.warning("OPENAI_API_KEY –Ω–µ –∑–∞–¥–∞–Ω, –∑–∞–ø—Ä–æ—Å—ã –∫ OpenRouter –Ω–µ –±—É–¥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å.")
        self.client = OpenAI(api_key=self.api_key, base_url="https://openrouter.ai/api/v1")

    async def ask(
        self,
        question: str,
        context: str,
        max_retries: int = CONFIG["retry_attempts"],
    ) -> str:
        if not self.api_key:
            return "‚ùå –ù–µ –∑–∞–¥–∞–Ω –∫–ª—é—á OPENAI_API_KEY –¥–ª—è OpenRouter."

        for attempt in range(max_retries):
            try:
                messages = [
                    {
                        "role": "system",
                        "content": (
                            "–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. "
                            "–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ —Ç–æ—á–Ω–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."
                        ),
                    },
                    {
                        "role": "user",
                        "content": f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:\n{context}\n\n–í–æ–ø—Ä–æ—Å: {question}",
                    },
                ]

                response = await asyncio.to_thread(
                    lambda: self.client.chat.completions.create(
                        model=self.model_id,
                        messages=messages,
                    )
                )
                return response.choices[0].message.content
            except APIError as e:
                logging.error(f"–û—à–∏–±–∫–∞ OpenRouter API: {e}")
                if attempt == max_retries - 1:
                    return f"‚ùå –û—à–∏–±–∫–∞ API (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {str(e)}"
                await asyncio.sleep(2 ** attempt)

        return "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç"


client = OpenRouterClient()


@lru_cache(maxsize=1)
def load_embedding_model() -> SentenceTransformer:
    return SentenceTransformer(CONFIG["embedding_model"])


class VectorKnowledgeBase:
    def __init__(self, db_path: str) -> None:
        self.db_path = db_path
        self.model = load_embedding_model()
        self.init_db()
        self.chunks, self.embeddings = self._load_chunks()

    def init_db(self) -> None:
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS chunks (
                id INTEGER PRIMARY KEY,
                filename TEXT,
                chunk_text TEXT,
                chunk_index INTEGER,
                doc_type TEXT,
                embedding BLOB,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
            """
        )
        conn.commit()
        conn.close()

    def _chunk_text(self, text: str, chunk_size: int = CONFIG["chunk_size"]) -> List[str]:
        words = text.split()
        return [" ".join(words[i : i + chunk_size]) for i in range(0, len(words), chunk_size)]

    def add_document(self, filename: str, content: str, doc_type: str) -> None:
        chunks = self._chunk_text(content)
        if not chunks:
            logging.warning(f"–î–æ–∫—É–º–µ–Ω—Ç {filename} –ø—É—Å—Ç–æ–π, –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω –≤ –±–∞–∑—É.")
            return

        embeddings = self.model.encode(chunks)
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        for i, (chunk, emb) in enumerate(zip(chunks, embeddings)):
            cursor.execute(
                """
                INSERT OR REPLACE INTO chunks (filename, chunk_text, chunk_index, doc_type, embedding)
                VALUES (?, ?, ?, ?, ?)
                """,
                (filename, chunk, i, doc_type, emb.tobytes()),
            )

        conn.commit()
        conn.close()
        self.chunks, self.embeddings = self._load_chunks()
        logging.info(f"–î–æ–∫—É–º–µ–Ω—Ç {filename} –¥–æ–±–∞–≤–ª–µ–Ω –≤ –±–∞–∑—É: {len(chunks)} —á–∞–Ω–∫–æ–≤.")

    def _load_chunks(self) -> Tuple[List[str], np.ndarray]:
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT chunk_text, embedding FROM chunks")
        chunks: List[str] = []
        embeddings: List[np.ndarray] = []

        for chunk_text, emb_bytes in cursor.fetchall():
            chunks.append(chunk_text)
            embeddings.append(np.frombuffer(emb_bytes, dtype=np.float32))

        conn.close()

        if embeddings:
            return chunks, np.array(embeddings)
        return [], np.array([])

    def search(self, query: str, top_k: int = CONFIG["top_k"]) -> str:
        if not self.chunks or self.embeddings.size == 0:
            return "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞"

        query_emb = self.model.encode([query])
        similarities = cosine_similarity(query_emb, self.embeddings)[0]
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        context = "\n\n".join([self.chunks[i] for i in top_indices])

        # max_context_tokens –∫–∞–∫ –ª–∏–º–∏—Ç –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        return context[: CONFIG["max_context_tokens"]]


kb = VectorKnowledgeBase(CONFIG["db_path"])


@lru_cache(maxsize=128)
def get_ocr_reader():
    return easyocr.Reader(["ru", "en"], gpu=False)


async def process_document(filepath: str) -> Tuple[str, str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (content, doc_type)."""
    filename = Path(filepath).name
    cache_key = f"ocr_{filename}"

    if hasattr(process_document, "cache") and cache_key in process_document.cache:
        return process_document.cache[cache_key]

    if filepath.lower().endswith(".txt"):
        async with aiofiles.open(filepath, "r", encoding="utf-8") as f:
            content = await f.read()
        doc_type = "txt"

    elif filepath.lower().endswith(".pdf"):
        try:
            # 1. –ü—Ä–æ–±—É–µ–º –≤—ã—Ç–∞—â–∏—Ç—å —Ç–µ–∫—Å—Ç —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞–∫ –µ—Å—Ç—å
            reader = PdfReader(filepath)
            text_pages = [page.extract_text() or "" for page in reader.pages]
            content = " ".join(text_pages)
            doc_type = "pdf"

            # 2. –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –æ—á–µ–Ω—å –º–∞–ª–æ, –¥–æ–±–∏—Ä–∞–µ–º –µ–≥–æ —á–µ—Ä–µ–∑ OCR –ø–æ –≤—Å–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
            if len(content.strip()) < 100:  # –ø–æ—Ä–æ–≥ –º–æ–∂–Ω–æ –ø–æ–¥—Å—Ç—Ä–æ–∏—Ç—å –ø–æ–¥ —Å–µ–±—è
                ocr_reader = get_ocr_reader()
                images = convert_from_path(filepath, poppler_path=CONFIG.get("poppler_path"))
                ocr_text = " ".join(
                    " ".join(d[1] for d in ocr_reader.readtext(img)) for img in images
                )
                # –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–æ, —á—Ç–æ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–∞–∫ —Ç–µ–∫—Å—Ç, —Å OCR
                content = (content + " " + ocr_text).strip()

        except Exception:
            # 3. –ï—Å–ª–∏ PdfReader –≤–æ–æ–±—â–µ —É–ø–∞–ª, —Å—Ä–∞–∑—É —á–∏—Ç–∞–µ–º pdf —á–µ—Ä–µ–∑ OCR
            ocr_reader = get_ocr_reader()
            images = convert_from_path(filepath, poppler_path=CONFIG.get("poppler_path"))
            content = " ".join(
                " ".join(d[1] for d in ocr_reader.readtext(img)) for img in images
            )
            doc_type = "pdf"

    else:
        ocr_reader = get_ocr_reader()
        result = ocr_reader.readtext(filepath)
        content = " ".join(detection[1] for detection in result)
        doc_type = "image"

    if CONFIG["ocr_cache"]:
        process_document.cache = getattr(process_document, "cache", {})
        process_document.cache[cache_key] = (content, doc_type)

    return content, doc_type


def save_answer_docx(question: str, answer: str) -> str:
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = Path(CONFIG["answers_dir"]) / f"answer_{timestamp}.docx"

    doc = Document()
    doc.add_heading("ü§ñ AI Document Assistant Pro", level=0)
    doc.add_heading(f"‚ùì –í–æ–ø—Ä–æ—Å: {question}", level=1)
    doc.add_paragraph(answer)
    doc.save(filename)

    return str(filename)


def web_search(query: str, max_results: int | None = None) -> str:
    """–ü–æ–∏—Å–∫ –≤ DuckDuckGo, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–∫–ª–µ–µ–Ω–Ω—ã–µ —Å–Ω–∏–ø–ø–µ—Ç—ã."""
    if max_results is None:
        max_results = CONFIG.get("web_max_results", 3)

    try:
        results = []
        with DDGS() as ddgs:
            for r in ddgs.text(query, max_results=max_results):
                snippet = r.get("body") or r.get("snippet") or ""
                if snippet:
                    results.append(snippet)
        return "\n\n".join(results)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –≤–µ–±-–ø–æ–∏—Å–∫–∞: {e}")
        return ""


async def ask_ai(question: str) -> str:
    with st.spinner("ü§ñ –ò–ò –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã..."):
        doc_context = kb.search(question)

        use_web = st.session_state.get("use_web_search", CONFIG["use_web_search_default"])
        min_chars = st.session_state.get(
            "min_doc_context_chars", CONFIG["min_doc_context_chars"]
        )
        min_chunks = st.session_state.get("min_doc_chunks", CONFIG["min_doc_chunks"])
        web_max_results = st.session_state.get(
            "web_max_results", CONFIG["web_max_results"]
        )

        web_context = ""
        if use_web:
            few_chunks = len(kb.chunks) < min_chunks
            short_context = doc_context == "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞" or len(doc_context) < min_chars

            if few_chunks or short_context:
                web_context = web_search(question, max_results=web_max_results)

        if web_context:
            full_context = (
                f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:\n{doc_context}\n\n"
                f"–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞:\n{web_context}"
            )
        else:
            full_context = doc_context

        answer = await client.ask(question, full_context)
    return answer


def main() -> None:
    st.set_page_config(
        page_title=CONFIG["app_name"],
        page_icon="ü§ñ",
        layout="wide",
    )

    st.title(f"ü§ñ {CONFIG['app_name']}")
    st.markdown("---")

    with st.sidebar:
        st.header("üìÅ –î–æ–∫—É–º–µ–Ω—Ç—ã")
        uploaded_files = st.file_uploader(
            "–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã",
            accept_multiple_files=True,
            type=["txt", "pdf", "png", "jpg", "jpeg"],
        )

        if uploaded_files:
            for file in uploaded_files:
                filepath = Path(CONFIG["documents_dir"]) / file.name
                with open(filepath, "wb") as f:
                    f.write(file.getbuffer())

                try:
                    content, doc_type = asyncio.run(process_document(str(filepath)))
                    kb.add_document(file.name, content, doc_type)
                    st.success(f"‚úÖ {file.name} –∑–∞–≥—Ä—É–∂–µ–Ω –∏ –¥–æ–±–∞–≤–ª–µ–Ω –≤ –±–∞–∑—É ({doc_type})")
                except Exception as e:
                    logging.exception(e)
                    st.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file.name}: {e}")

        st.header("üåê –ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–ø–æ–∏—Å–∫")

        use_web_search = st.checkbox(
            "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç‚Äë–ø–æ–∏—Å–∫",
            value=CONFIG.get("use_web_search_default", True),
        )

        min_doc_context_chars = st.number_input(
            "–ú–∏–Ω. –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (—Å–∏–º–≤–æ–ª—ã)",
            min_value=0,
            value=int(CONFIG.get("min_doc_context_chars", 500)),
            step=100,
        )

        min_doc_chunks = st.number_input(
            "–ú–∏–Ω. —á–∏—Å–ª–æ —á–∞–Ω–∫–æ–≤",
            min_value=0,
            value=int(CONFIG.get("min_doc_chunks", 1)),
            step=1,
        )

        web_max_results = st.number_input(
            "–†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–µ–±-–ø–æ–∏—Å–∫–∞",
            min_value=1,
            max_value=10,
            value=int(CONFIG.get("web_max_results", 3)),
            step=1,
        )

        st.session_state.use_web_search = use_web_search
        st.session_state.min_doc_context_chars = int(min_doc_context_chars)
        st.session_state.min_doc_chunks = int(min_doc_chunks)
        st.session_state.web_max_results = int(web_max_results)

        st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
        st.info(f"–ß–∞–Ω–∫–æ–≤ –≤ –±–∞–∑–µ: {len(kb.chunks)}")

        if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"):
            conn = sqlite3.connect(CONFIG["db_path"])
            conn.execute("DELETE FROM chunks")
            conn.commit()
            conn.close()
            kb.chunks, kb.embeddings = [], np.array([])
            st.success("–ë–∞–∑–∞ –æ—á–∏—â–µ–Ω–∞!")

    col1, col2 = st.columns([3, 1])

    with col1:
        question = st.text_area("‚ùì –ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º:", height=100, key="question")

    with col2:
        if st.button("üöÄ –°–ø—Ä–æ—Å–∏—Ç—å –ò–ò", type="primary", use_container_width=True):
            if question:
                answer = asyncio.run(ask_ai(question))
                st.session_state.last_answer = answer
                st.session_state.last_question = question

                st.write("### ‚úÖ –û—Ç–≤–µ—Ç –ò–ò:")
                st.write(answer)

                if st.button("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç–≤–µ—Ç –≤ DOCX"):
                    path = save_answer_docx(question, answer)
                    st.success(f"–û—Ç–≤–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {path}")
            else:
                st.warning("–í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å.")


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "--console":
        print("–ö–æ–Ω—Å–æ–ª—å–Ω—ã–π —Ä–µ–∂–∏–º (–∑–∞–ø—É—Å–∫ Web UI: streamlit run dialog_pro.py)")
    else:
        main()
