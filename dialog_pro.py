import os
import json
import asyncio
import logging
import aiofiles
from pathlib import Path
from typing import List, Tuple
from datetime import datetime
import streamlit as st
import sqlite3
from dotenv import load_dotenv
from openai import OpenAI, APIError
from docx import Document

from duckduckgo_search import DDGS
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import warnings
import fitz

warnings.filterwarnings("ignore")
load_dotenv()

# CONFIG
CONFIG = {
    'app_name': 'AI Document Assistant Pro v2.0',
    'max_context_tokens': 4000,
    'chunk_size': 150,
    'chunk_overlap': 30,
    'top_k': 3,
    'embedding_model': 'paraphrase-multilingual-MiniLM-L12-v2',
    'openrouter_model': 'meta-llama/llama-3.2-3b-instruct:free',
    'documents_dir': 'documents',
    'answers_dir': 'answers',
    'db_path': 'knowledgebase_pro.db',
    'retry_attempts': 3,
    'use_web_search_default': True,
    'min_doc_context_chars': 500,
    'min_doc_chunks': 1,
    'web_max_results': 3
}

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
os.makedirs(CONFIG['documents_dir'], exist_ok=True)
os.makedirs(CONFIG['answers_dir'], exist_ok=True)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


@st.cache_resource
def load_embedding_model():
    st.info('üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (~30 —Å–µ–∫)...')
    model = SentenceTransformer(CONFIG['embedding_model'], device='cpu')
    st.success('‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞!')
    return model


class OpenRouterClient:
    def __init__(self, model_id: str = None):
        self.api_key = os.getenv('OPENAI_API_KEY')
        self.model_id = model_id or CONFIG['openrouter_model']
        if not self.api_key:
            st.error("‚ùå OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω! –°–æ–∑–¥–∞–π—Ç–µ .env —Ñ–∞–π–ª")
        self.client = OpenAI(api_key=self.api_key, base_url="https://openrouter.ai/api/v1")

    async def ask(self, question: str, context: str) -> str:
        if not self.api_key:
            return "‚ùå –ù–∞—Å—Ç—Ä–æ–π—Ç–µ OPENAI_API_KEY –≤ .env"

        try:
            messages = [
                {"role": "system",
                 "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç."},
                {"role": "user", "content": f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n–í–æ–ø—Ä–æ—Å: {question}"}
            ]
            response = await asyncio.to_thread(
                self.client.chat.completions.create,
                model=self.model_id,
                messages=messages,
                max_tokens=CONFIG['max_context_tokens']
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞ API: {str(e)}"


class VectorKnowledgeBase:
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.model = load_embedding_model()
        self.init_db()
        self.chunks, self.embeddings = self.load_chunks()

    def init_db(self):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS chunks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                filename TEXT,
                chunk_text TEXT,
                chunk_index INTEGER,
                doc_type TEXT,
                embedding BLOB
            )
        """)
        conn.commit()
        conn.close()

    def chunk_text(self, text: str) -> List[str]:
        if len(text) < 200:  # –ú–∞–ª–µ–Ω—å–∫–∏–π —Ç–µ–∫—Å—Ç —Ü–µ–ª–∏–∫–æ–º
            return [text] if len(text) > 50 else []

        # 1. –ü–æ —Å–ª–æ–≤–∞–º (–æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥)
        words = text.split()
        chunks = []
        step = max(1, CONFIG['chunk_size'] - CONFIG['chunk_overlap'])  # –ú–∏–Ω–∏–º—É–º 1!

        for i in range(0, len(words), step):
            chunk = ' '.join(words[i:i + CONFIG['chunk_size']])
            if len(chunk) > 50:
                chunks.append(chunk)

        # 2. Fallback: –ø–æ —Å–∏–º–≤–æ–ª–∞–º –µ—Å–ª–∏ —Å–ª–æ–≤ –º–∞–ª–æ
        if len(chunks) == 0 and len(text) > 200:
            for i in range(0, len(text), 800):
                chunk = text[i:i + 800].strip()
                if len(chunk) > 100:
                    chunks.append(chunk)

        print(f"üîç CHUNKING: {len(text.split())} —Å–ª–æ–≤ ‚Üí {len(chunks)} —á–∞–Ω–∫–æ–≤")
        return chunks[:50]  # –ú–∞–∫—Å–∏–º—É–º 50 —á–∞–Ω–∫–æ–≤ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç

    def add_document(self, filename: str, content: str, doc_type: str):
        # ‚úÖ –§–∏–∫—Å–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ
        if not filename or len(filename) > 100:
            filename = "document.pdf"

        clean_filename = Path(filename).stem[:50] + f".{doc_type}"  # Kolobkov.pdf
        print(f"üîç SAVE: '{clean_filename}' ‚Üí {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ ‚Üí {len(self.chunk_text(content))} —á–∞–Ω–∫–æ–≤")

        chunks = self.chunk_text(content)
        if not chunks:
            print(f"‚ùå –ü–£–°–¢–´–ï –ß–ê–ù–ö–ò –¥–ª—è {clean_filename}")
            return

        embeddings = self.model.encode(chunks)
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        for i, (chunk, emb) in enumerate(zip(chunks, embeddings)):
            cursor.execute("""
                INSERT OR REPLACE INTO chunks (filename, chunk_text, chunk_index, doc_type, embedding)
                VALUES (?, ?, ?, ?, ?)
            """, (clean_filename, chunk, i, doc_type, emb.tobytes()))
        conn.commit()
        conn.close()
        self.chunks, self.embeddings = self.load_chunks()
        st.success(f"‚úÖ {clean_filename}: {len(chunks)} —á–∞–Ω–∫–æ–≤")

    def load_chunks(self):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT chunk_text, embedding FROM chunks")
        chunks = []
        embeddings = []
        for chunk_text, emb_bytes in cursor.fetchall():
            chunks.append(chunk_text)
            embeddings.append(np.frombuffer(emb_bytes, dtype=np.float32))
        conn.close()
        return chunks, np.array(embeddings) if embeddings else np.array([])

    def search(self, query: str, top_k: int = CONFIG['top_k']) -> str:
        if len(self.chunks) == 0:
            return "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã."

        query_emb = self.model.encode([query])
        similarities = cosine_similarity(query_emb, self.embeddings)[0]
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        context = '\n'.join([self.chunks[i] for i in top_indices])
        return context[:CONFIG['max_context_tokens']]


async def process_document(filepath: str) -> Tuple[str, str]:
    filename = Path(filepath).name
    content = ""

    if filepath.lower().endswith('.txt'):
        async with aiofiles.open(filepath, 'r', encoding='utf-8') as f:
            content = await f.read()
        doc_type = 'txt'
        logging.info(f"TXT {filename}: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")

    elif filepath.lower().endswith('.pdf'):
        try:
            doc = fitz.open(filepath)
            pages_text = []
            for page_num in range(len(doc)):
                page = doc[page_num]
                text = page.get_text()
                pages_text.append(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num + 1}:\n{text}")  # ‚úÖ –í–°–ï —Å—Ç—Ä–∞–Ω–∏—Ü—ã!

            content = '\n\n---\n\n'.join(pages_text)
            #doc.close()
            doc_type = 'pdf'
            logging.info(f"PDF {filename}: {len(doc)} —Å—Ç—Ä–∞–Ω–∏—Ü, {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")


        except Exception as e:
            content = f"–û—à–∏–±–∫–∞ PDF: {e}"
            doc_type = 'pdf_error'

    elif filepath.lower().endswith('.docx'):
        try:
            doc = Document(filepath)
            paragraphs = [para.text for para in doc.paragraphs if para.text.strip()]
            content = '\n\n'.join(paragraphs) if paragraphs else "–ü—É—Å—Ç–æ–π DOCX"
            doc_type = 'docx'
            logging.info(f"DOCX {filename}: {len(paragraphs) if paragraphs else 0} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤")
        except Exception as e:
            content = f"–û—à–∏–±–∫–∞ DOCX: {e}"
            doc_type = 'docx_error'

    else:
        content = "–ü–æ–¥–¥–µ—Ä–∂–∫–∞: TXT/PDF/DOCX"
        doc_type = 'unsupported'

    return content, doc_type


def save_answer_docx(question: str, answer: str) -> str:
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
    filename = Path(CONFIG['answers_dir']) / f"answer_{timestamp}.docx"
    doc = Document()
    doc.add_heading(CONFIG['app_name'], level=1)
    doc.add_heading(f"–í–æ–ø—Ä–æ—Å: {question}", level=2)
    doc.add_paragraph(f"–û—Ç–≤–µ—Ç:\n{answer}")
    doc.save(filename)
    return str(filename)


def web_search(query: str, max_results: int = CONFIG['web_max_results']) -> str:
    try:
        with DDGS() as ddgs:
            results = [r.get('body') or r.get('snippet', '')
                       for r in ddgs.text(query, max_results=max_results)]
        return '\n'.join(results)
    except:
        return ""


async def ask_ai(client, kb, question: str) -> str:
    use_web = st.session_state.get('use_web_search', True)
    doc_context = kb.search(question)

    # –í–µ–±-–ø–æ–∏—Å–∫ –µ—Å–ª–∏ –º–∞–ª–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    min_chars = st.session_state.get('min_doc_context_chars', 500)
    few_chunks = len(kb.chunks) < 2
    short_context = len(doc_context) < min_chars

    web_context = ""
    if use_web and (few_chunks or short_context):
        web_context = web_search(question)

    full_context = f"{doc_context}\n\n{'–î–æ–ø. –≤–µ–±: ' + web_context if web_context else ''}"
    return await client.ask(question, full_context)


def main():
    st.set_page_config(page_title=CONFIG['app_name'], layout='wide', initial_sidebar_state='expanded')
    st.title(CONFIG['app_name'])
    st.markdown("---")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    if 'client' not in st.session_state:
        st.session_state.client = OpenRouterClient()
    if 'kb' not in st.session_state:
        st.session_state.kb = VectorKnowledgeBase(CONFIG['db_path'])

    client = st.session_state.client
    kb = st.session_state.kb

    with st.spinner('üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è...'):
        st.success(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –ß–∞–Ω–∫–æ–≤: {len(kb.chunks)}")
        # Sidebar —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        col1, col2 = st.columns(2)
        with col1:
            st.metric("üìä –ß–∞–Ω–∫–æ–≤", len(kb.chunks))
        with col2:
            unique_files = len(
                set(chunk.split('filename=')[1].split(',')[0] for chunk in kb.chunks if 'filename=' in chunk))
            st.metric("üìÑ –§–∞–π–ª–æ–≤", unique_files)

        if st.button("üìã –ü–æ–∫–∞–∑–∞—Ç—å —Ñ–∞–π–ª—ã"):
            files = list(set(Path(c).parent.name for c in kb.chunks))
            st.write(f"**–§–∞–π–ª—ã –≤ –±–∞–∑–µ:** {', '.join(files)}")

    # Sidebar
    with st.sidebar:
        st.header("üìÅ –î–æ–∫—É–º–µ–Ω—Ç—ã")
        uploaded_files = st.file_uploader(
            "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã",
            accept_multiple_files=True,
            type=['txt', 'pdf', 'docx']
        )

        if uploaded_files:
            for file in uploaded_files:
                filepath = Path(CONFIG['documents_dir']) / file.name
                with open(filepath, 'wb') as f:
                    f.write(file.getbuffer())
                try:
                    content, doc_type = asyncio.run(process_document(str(filepath)))
                    kb.add_document(file.name, content, doc_type)
                    st.session_state.kb = VectorKnowledgeBase(CONFIG['db_path'])  # –û–±–Ω–æ–≤–∏—Ç—å —Å–µ—Å—Å–∏—é
                except Exception as e:
                    st.error(f"‚ùå {file.name}: {e}")

        st.header("ü§ñ –ú–æ–¥–µ–ª—å –ò–ò")
        model_options = [
        "allenai/molmo-2-8b:free",
        "bytedance-seed/seedream-4.5",
        "xiaomi/mimo-v2-flash:free",
        "mistralai/devstral-2512:free",
        "sourceful/riverflow-v2-max-preview",
        "meta-llama/llama-3.2-3b-instruct",  # ‚úÖ –ë–ê–ó–û–í–ê–Ø - –≤—Å–µ–≥–¥–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç
        ]

        selected_model = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ:", model_options, key="model")
        if selected_model != client.model_id:
            st.session_state.client = OpenRouterClient(selected_model)
            st.rerun()

        st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
        st.session_state.use_web_search = st.checkbox("üîç –í–µ–±-–ø–æ–∏—Å–∫", value=True)
        st.session_state.min_doc_context_chars = st.number_input("–ú–∏–Ω. —Å–∏–º–≤–æ–ª–æ–≤",
                                                                 value=CONFIG['min_doc_context_chars'], min_value=0,
                                                                 step=100)

        if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å –±–∞–∑—É"):
            conn = sqlite3.connect(CONFIG['db_path'])
            conn.execute("DELETE FROM chunks")
            conn.commit()
            conn.close()
            st.session_state.kb = VectorKnowledgeBase(CONFIG['db_path'])
            st.success("‚úÖ –ë–∞–∑–∞ –æ—á–∏—â–µ–Ω–∞!")
            st.rerun()

    # –ì–ª–∞–≤–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
    col1, col2 = st.columns([3, 1])
    with col1:
        question = st.text_area("‚ùì –í–æ–ø—Ä–æ—Å –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º:", height=100, key="question")
    with col2:
        if st.button("üöÄ –°–ø—Ä–æ—Å–∏—Ç—å –ò–ò", type="primary") and question:
            with st.spinner('ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è...'):
                answer = asyncio.run(ask_ai(client, kb, question))
                st.markdown(f"**–û—Ç–≤–µ—Ç:**\n{answer}")
                st.session_state.last_answer = answer

    if st.session_state.get('last_answer') and st.button("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å DOCX"):
        path = save_answer_docx(st.session_state.last_question or question,
                                st.session_state.last_answer)
        st.success(f"‚úÖ {path}")



if __name__ == "__main__":
    main()
